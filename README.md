# Deep-Learning-Hardcoding-backpropogation-in-Numpy-and-training-on-MNIST-dataset
In this project, am L-layered deep neural network is implemented by hardcoding and developing the equations of backpropogation in Numpy. The network so developed is then trained on the MNIST dataset.

# Hardcoding of backpropogation in Numpy
Backpropogation in neural networks is the mechanism through which the weights and biases of the neural network are changed in response to predictions and errors on those predictions made by the neural network. 

The mathematical equations of this mechanism are hardcoded in Numpy by building custom functions.

# Evaluation the network on MNIST data set
The MNIST dataset contains scanned images of handwritten digits, along with their correct classification labels (between 0-9).
The network built by hardcoding of backpropogation algorithm in Numpy is trained on the training subset of MNIST data set. It is then evaluated on test data setgiving an accuracy of 86.7%.
